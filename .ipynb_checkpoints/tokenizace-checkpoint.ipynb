{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fb326a",
   "metadata": {},
   "source": [
    "# Tokenizace\n",
    "\n",
    "V tomto notebooku si v několika krocích vytvoříme program pro tokenizaci textu. Použijeme k tomu pouze základní metody Pythonu a knihovnu re pro regulární výrazy.\n",
    "\n",
    "Tokenizace je jedním z běžných problémů zpracování přirozeného jazyka. S tokenizací se setkáme v korpusech, v syntéze řeči, během strojového učení apod. Jedná se o rozdělení textu na menší části, přičemž tyto části, kterým říkáme tokeny, mohou být věty, slova, či jinak definované řetězce. Jelikož se jedná o běžnou součást NLP (zpracování přirozeného jazyka), mohli bychom předpokládat, že tokenizace patří mezi dobře popsané problémy, který není třeba řešit. Jak si ale ukážeme dále, s tokenizací to není tak jednoduché a velmi záleží na tom, pro co data zpracováváme a jak definujeme samotné tokeny.\n",
    "\n",
    "## 1 Problémy tokenizace\n",
    "\n",
    "Pokud se zaměříme na tokenizaci slov, musíme se rozhodnout, jak budeme ke slovům přistupovat. Pokud narazíme na slovní spojení \"Jižní Amerika\", budeme jej tokenizovat jako dvě různá slova \"Jižní\" a \"Amerika\", nebo zvolíme variantu \"Jižní Amerika\", jelikož tato dvě slova spolu tvoří význam? Jak v anglickém textu budeme tokenizovat \"don't\"? Přikloníme se k variantě \"don't\", \"do\" a \"n't\", \"dont\", nebo úplně jiné variantě? Pokud narazíme na slovo \"překladatel-tlumočník\", rozhodneme se ho ponechat se spojovníkem nebo rozdělit do slov \"překladatel\" a \"tlumočník\"?  A co slovo \"dvou-\" ve spojení \"dvou‑ až třílůžkový pokoj\"? Jak budeme nakládat s daty, čísly, zkratkami, internetovými adresami, interpunkcí apod.?\n",
    "\n",
    "Seznam by mohl dále pokračovat. Z předchozích příkladů můžeme vidět, že problémů definice slova (tokenu), je spousta, přičemž žádný z uvedených příkladů nemá jedno správné řešení, všechna uvedená by byla vhodná v různých kontextech.\n",
    "\n",
    "V našem případě nebude důležité vyřešit všechny tyto problémy. Zaměříme se pouze na ty, které se vyskytují v textu, jenž budeme tokenizovat.\n",
    "\n",
    "## 2 S čím budeme pracovat\n",
    "\n",
    "### 2.1 re\n",
    "\n",
    "V tomto notebooku použijeme modul re, který slouží pro práci s regulárními výrazy.\n",
    "\n",
    "Více informací na [re](https://docs.python.org/3/library/re.html).\n",
    "\n",
    "Pro připomenutí regulárních výrazů vám může pomoct tento [cheatsheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/). Než výrazy implementujete do svého kódu, můžete si jejich funkčnost vyzkoušet např. na [regex101](https://regex101.com/) (nezapomeňte si v levém boxu přepnout flavor na Python).\n",
    "\n",
    "## 3 Instalace\n",
    "\n",
    "Modul re je součástí tzv. The Python Standard Library (standarní knihovny Pythonu), není tedy nutné nic instalovat.\n",
    "\n",
    "Více informací na [The Python Standard Library](https://docs.python.org/3/library/).\n",
    "\n",
    "## 4 Import knihoven a modulů\n",
    "\n",
    "Než budeme moct začít s psaním programu, musíme importovat všechny knihovny a moduly, které budeme potřebovat. Patří mezi ně:\n",
    "\n",
    "- re\n",
    "\n",
    "Spusťte následující buňku, modul re se importuje.\n",
    "\n",
    "**Poznámka:** Po každém zavření a otevření notebooku je nutné všechen kód (tj. i importování) spustit znovu. Výsledky sice zůstanou zobrazeny, obsah proměnných však v paměti nezůstává."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e316e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fffb90",
   "metadata": {},
   "source": [
    "## 5 Nejjednodušší tokenizátor\n",
    "\n",
    "Nejjednodušší tokenizátor v Pythonu naprogramujeme na třech řádcích díky metodě `split`. Text rozdělíme na slova pomocí mezer.\n",
    "\n",
    "Nejprve vyzveme uživatele k zadání textu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb852ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zadejte text pro tokenizaci:      V polovině ledna se v médiích objevila zpráva, že předsedkyně Poslanecké sněmovny Markéta Pekarová Adamová shání stážistu. „Stážistce či stážistovi nabízíme aktivní zapojení do práce v oblasti mezinárodních vztahů a diplomatických aktivit předsedkyně, seznámení se s chodem kanceláře předsedkyně a také s prostorami a fungováním Poslanecké sněmovny,“ stojí v inzerátu.\n"
     ]
    }
   ],
   "source": [
    "text = input('Zadejte text pro tokenizaci: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f497c8",
   "metadata": {},
   "source": [
    "Text poté tokenizujeme pomocí metody `split`. Pokud této metodě nespecifikujeme, podle čeho má text rozdělit, použije k rozdělení slov mezeru. V tomto případě je toto chování, které požadujeme, argumenty metody tedy necháme prázdné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3127f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efaa69a",
   "metadata": {},
   "source": [
    "Tokenizovaný text vypíšeme.\n",
    "\n",
    "**Poznámka:** Tímto způsobem lze text vypsat pouze v prostředí Jupyter Notebook. Pokud budete programovat v jiných prostředích, použijte standardní `print(text)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "153dbc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V',\n",
       " 'polovině',\n",
       " 'ledna',\n",
       " 'se',\n",
       " 'v',\n",
       " 'médiích',\n",
       " 'objevila',\n",
       " 'zpráva,',\n",
       " 'že',\n",
       " 'předsedkyně',\n",
       " 'Poslanecké',\n",
       " 'sněmovny',\n",
       " 'Markéta',\n",
       " 'Pekarová',\n",
       " 'Adamová',\n",
       " 'shání',\n",
       " 'stážistu.',\n",
       " '„Stážistce',\n",
       " 'či',\n",
       " 'stážistovi',\n",
       " 'nabízíme',\n",
       " 'aktivní',\n",
       " 'zapojení',\n",
       " 'do',\n",
       " 'práce',\n",
       " 'v',\n",
       " 'oblasti',\n",
       " 'mezinárodních',\n",
       " 'vztahů',\n",
       " 'a',\n",
       " 'diplomatických',\n",
       " 'aktivit',\n",
       " 'předsedkyně,',\n",
       " 'seznámení',\n",
       " 'se',\n",
       " 's',\n",
       " 'chodem',\n",
       " 'kanceláře',\n",
       " 'předsedkyně',\n",
       " 'a',\n",
       " 'také',\n",
       " 's',\n",
       " 'prostorami',\n",
       " 'a',\n",
       " 'fungováním',\n",
       " 'Poslanecké',\n",
       " 'sněmovny,“',\n",
       " 'stojí',\n",
       " 'v',\n",
       " 'inzerátu.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdc849",
   "metadata": {},
   "source": [
    "Jak si můžeme všimnout, text se tokenizoval, dokonce jsme se díky metodě `split` zbavili počátečních mezer. Hlavní problém je ale s interpunkcí. V našem tokenizátoru určitě nebudeme chtít mít interpunkci jako součást předchozího slova, samotná metoda `split` nám tedy nebude stačit.\n",
    "\n",
    "V případě tokenizace není ideální ani uložení textu do proměnné přes metodu `input`. Zejména v případě, kdy budeme chtít tokenizovat dlouhé texty, se nám bude hodit jiný přístup, abychom text nemuseli kopírovat a následně jej vkládat.\n",
    "\n",
    "To stejné platí o přístupu k výslednému tokenizovanému textu. Může se nám hodit nechat jej v proměnné a dále ho zpracovávat, měli bychom ale být schopni výstup uložit do souboru.\n",
    "\n",
    "Než budeme upravovat náš tokenizátor, naučíme se, jak pomocí Pythonu otevřít soubor a přečíst jeho obsah. Ukážeme si, jak obsah souboru přepsat nebo k němu přidat další řádky textu. V neposlední řadě zjistíme, jak vytvořit úplně nový soubor.\n",
    "\n",
    "### 5.1 Otevření souboru\n",
    "\n",
    "K otevření souboru v Pythonu slouží metoda `open`. Požaduje dva parametry:\n",
    "\n",
    "1. název souboru (pokud se soubor nachází ve stejné složce jako program, stačí pouze název s formátem, pokud se soubor nachází jinde, je nutné specifikovat celou cestu),\n",
    "2. mód, v jakém se soubor otevře (v našem případě `r`= read = ke čtení).\n",
    "\n",
    "Následující buňku pouze spusťte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190afda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open('uryvek.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59cb42d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='uryvek.txt' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856d4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = txt.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd63ca1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      V polovině ledna se v médiích objevila zpráva, že předsedkyně Poslanecké sněmovny Markéta Pekarová Adamová shání stážistu. „Stážistce či stážistovi nabízíme aktivní zapojení do práce v oblasti mezinárodních vztahů a diplomatických aktivit předsedkyně, seznámení se s chodem kanceláře předsedkyně a také s prostorami a fungováním Poslanecké sněmovny,“ stojí v inzerátu.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840743b",
   "metadata": {},
   "source": [
    "### 5.2 Zavření souboru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bbc91",
   "metadata": {},
   "source": [
    "### 5.3 Zápis do již existujícího souboru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6acb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c6e9d84",
   "metadata": {},
   "source": [
    "### 5.4 Vytvoření nového souboru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73bb9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0e1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2957ac2b",
   "metadata": {},
   "source": [
    "V další sekci notebooku proto tokenizátor upravíme pomocí regulárních výrazů. Text pro tokenizaci budeme načítat ze souboru a ukážeme si různé způsoby práce s výsledným tokenizovaným textem.\n",
    "\n",
    "## 6 Tokenizátor s použitím regulárních výrazů\n",
    "\n",
    "Naprogramujte tokenizátor, který "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f79f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba92123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb236d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e3c61cb",
   "metadata": {},
   "source": [
    "Kod z Programming for Linguists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8818fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This program works as a simple segmentation and tokenization tool. It reads a text file\n",
    "and then outputs the text, one sentence per line, all tokens separated by space. Tokens are:\n",
    "words, abbreviations ('e.g.', 'Mr.', 'Mrs.', and 'U.S.A.' types), abbreviated forms of verbs (isn't),\n",
    "punctuation, quotes, brackets, numbers ('55', '55.5', '55,5', '55,000.55', and '55,000.55' types),\n",
    "numbers together with currencies or percentage signs.\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def prepare_raw(data):\n",
    "    \"\"\"Read file, remove file's newline characters and replace them by space. Then split\n",
    "    the text using space.\"\"\"\n",
    "\n",
    "    data = open(data, \"r\")\n",
    "    raw = data.read()\n",
    "    raw = raw.replace(\"\\n\", \" \")\n",
    "    raw = raw.split(\" \")\n",
    "    raw = raw[:-1] # used because of standard behaviour of .split() method \n",
    "                   # which appends an empty string at the end of the list\n",
    "    data.close()\n",
    "    return raw\n",
    "\n",
    "\n",
    "def newline_char(raw):\n",
    "    \"\"\"Append newline characters at the end of the sentences.\"\"\"\n",
    "\n",
    "    abbr = re.compile(r'\\b(?:[A-Za-z]\\.)+|\\b(?:[A-Z][a-z]{1,2}\\.)+|(?:[A-Z]\\.)+') # matches abbreviations such as e.g., Dr., Mrs., or U.S.A.\n",
    "    end_punct = re.compile(r'(\\.|\\?|\\!)$') # matches .?! punctuation marks (at the end of sentences)\n",
    "    end_br = re.compile(r'[\\.\\?\\!]+[\\)\\'\"]$') # matches .?! punctuation inside brackets and quotation marks\n",
    "\n",
    "    for i in range(len(raw)):\n",
    "       if (re.search(end_punct, raw[i]) or re.search(end_br, raw[i])) and not re.search(abbr, raw[i]):\n",
    "            raw[i] = raw[i] + \"\\n\"\n",
    "    return raw\n",
    "\n",
    "\n",
    "def sentence_list(raw):\n",
    "    \"\"\"Makes a list of lists. Each inner list contains a sentence.\"\"\"\n",
    "\n",
    "    sentence_per_line = []\n",
    "    sent = []\n",
    "    \n",
    "    for part in raw:\n",
    "        if part[-1:] != \"\\n\": # append the inner token to the sentence\n",
    "            sent.append(part)\n",
    "    \n",
    "        else:\n",
    "            sent.append(part[:-1]) # append the last word of the sentence to the sentence\n",
    "                                   # without the newline character\n",
    "            sentence_per_line.append(sent) # append the whole sentence to the list of all sentences\n",
    "            sent = [] # reset the variable for next sentence\n",
    "    \n",
    "    return sentence_per_line\n",
    "\n",
    "\n",
    "def tokenize(sentence_per_line):\n",
    "    \"\"\"Tokenize the sentences according to the rules mentioned below.\"\"\"\n",
    "\n",
    "    abbr = re.compile(r'\\b(?:[A-Za-z]\\.)+|\\b(?:[A-Z][a-z]{1,2}\\.)+|(?:[A-Z]\\.)+') # matches abbreviations such as e.g., Dr., Mrs., or U.S.A.\n",
    "    numbers = re.compile(r'(\\d+[.,]?\\d+)+') # matches decimal numbers with decimal period, or comma, or both (e.g. 2,000.3)\n",
    "    end_punct = re.compile(r'(\\.|\\?|\\!)$') # matches .?! punctuation marks (for the end of sentences)\n",
    "    end_br = re.compile(r'[\\.\\?\\!]+[\\)\\'\"]$') # matches .?! punctuation inside brackets and quotation marks\n",
    "    brq_front = re.compile(r'^[\\\"\\[\\(\\{\\']') # matches brackets and quotation marks at the beginning of the word\n",
    "    brq_back = re.compile(r'[\\\"\\]\\)\\}\\']$') # matches brackets and quotations marks at the end of the word\n",
    "\n",
    "    for lst in sentence_per_line:\n",
    "        for tok in lst:\n",
    "            if re.search(end_punct, tok) and len(tok) > 1 and not re.search(abbr, tok):\n",
    "                token1 = tok[:-1] # separate the word from the punctuation mark\n",
    "                end = tok[-1] # the punctuation mark itself\n",
    "                lst.remove(tok) # remove the last word\n",
    "                lst.append(token1) # append the separated word instead\n",
    "                lst.append(end) # append the separated punctuation instead\n",
    "\n",
    "            elif re.search(r'\\,$', tok) and len(tok) > 1:\n",
    "                where = lst.index(tok) # keep an index where to put the comma\n",
    "                token1 = tok[:-1] # the word itself\n",
    "                end = tok[-1] # the comma itself\n",
    "                lst.remove(tok) # remove the last word\n",
    "                lst.insert(where, token1) # insert the word at the index\n",
    "                lst.insert(where+1, end) # insert the comma after the word\n",
    "\n",
    "            elif re.search(end_br, tok) and len(tok) > 1: # search for punctuation inside brackets\n",
    "                token1 = tok[:-2] # word\n",
    "                end1 = tok[-2] # punctuation\n",
    "                end2 = tok[-1] # ending bracket\n",
    "                lst.remove(tok) # remove the last word\n",
    "                lst.append(token1) # append the separated word instead\n",
    "                lst.append(end1) # append the separated punctuation mark\n",
    "                lst.append(end2) # append the separated bracket\n",
    "        \n",
    "            elif re.search(brq_front, tok) and len(tok) > 1: # front brackets or quotations\n",
    "                where = lst.index(tok) # where to insert (it can be in the middle of the sentence)\n",
    "                token2 = tok[1:]\n",
    "                brq = tok[0]\n",
    "                lst.remove(tok)\n",
    "                lst.insert(where, brq) # we first want to insert the front bracket or quotation\n",
    "                lst.insert(where+1, token2) # then insert the word\n",
    "        \n",
    "            elif re.search(brq_back, tok) and len(tok) > 1: # back brackets or quotations\n",
    "                where = lst.index(tok) # where to insert\n",
    "                token1 = tok[:-1]\n",
    "                brq = tok[-1]\n",
    "                lst.remove(tok)\n",
    "                lst.insert(where, token1) # here we want the word first\n",
    "                lst.insert(where+1, brq) # and the bracket or the quote last\n",
    "\n",
    "    return sentence_per_line\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        inp = input(\"Enter a text to tokenize: \")\n",
    "        \n",
    "        if inp[-4:] != \".txt\": # the file needs to be .txt\n",
    "            print(\"NotTxtFileError: Enter a .txt file.\")\n",
    "            return 1\n",
    "        \n",
    "        raw = prepare_raw(inp)\n",
    "        newline_appended = newline_char(raw)\n",
    "        sentence_per_line = sentence_list(newline_appended)\n",
    "        tokens = tokenize(sentence_per_line)\n",
    "\n",
    "        for sentences in tokens: # print tokens divided by space\n",
    "            for toks in sentences:\n",
    "                print(toks, end=\" \")\n",
    "            print()\n",
    "\n",
    "        return 0\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"FileNotFoundError: Enter an existing file.\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
