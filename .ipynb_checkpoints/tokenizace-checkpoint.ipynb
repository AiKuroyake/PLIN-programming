{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fb326a",
   "metadata": {},
   "source": [
    "# Tokenizace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c61cb",
   "metadata": {},
   "source": [
    "Kod z Programming for Linguists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8818fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This program works as a simple segmentation and tokenization tool. It reads a text file\n",
    "and then outputs the text, one sentence per line, all tokens separated by space. Tokens are:\n",
    "words, abbreviations ('e.g.', 'Mr.', 'Mrs.', and 'U.S.A.' types), abbreviated forms of verbs (isn't),\n",
    "punctuation, quotes, brackets, numbers ('55', '55.5', '55,5', '55,000.55', and '55,000.55' types),\n",
    "numbers together with currencies or percentage signs.\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def prepare_raw(data):\n",
    "    \"\"\"Read file, remove file's newline characters and replace them by space. Then split\n",
    "    the text using space.\"\"\"\n",
    "\n",
    "    data = open(data, \"r\")\n",
    "    raw = data.read()\n",
    "    raw = raw.replace(\"\\n\", \" \")\n",
    "    raw = raw.split(\" \")\n",
    "    raw = raw[:-1] # used because of standard behaviour of .split() method \n",
    "                   # which appends an empty string at the end of the list\n",
    "    data.close()\n",
    "    return raw\n",
    "\n",
    "\n",
    "def newline_char(raw):\n",
    "    \"\"\"Append newline characters at the end of the sentences.\"\"\"\n",
    "\n",
    "    abbr = re.compile(r'\\b(?:[A-Za-z]\\.)+|\\b(?:[A-Z][a-z]{1,2}\\.)+|(?:[A-Z]\\.)+') # matches abbreviations such as e.g., Dr., Mrs., or U.S.A.\n",
    "    end_punct = re.compile(r'(\\.|\\?|\\!)$') # matches .?! punctuation marks (at the end of sentences)\n",
    "    end_br = re.compile(r'[\\.\\?\\!]+[\\)\\'\"]$') # matches .?! punctuation inside brackets and quotation marks\n",
    "\n",
    "    for i in range(len(raw)):\n",
    "       if (re.search(end_punct, raw[i]) or re.search(end_br, raw[i])) and not re.search(abbr, raw[i]):\n",
    "            raw[i] = raw[i] + \"\\n\"\n",
    "    return raw\n",
    "\n",
    "\n",
    "def sentence_list(raw):\n",
    "    \"\"\"Makes a list of lists. Each inner list contains a sentence.\"\"\"\n",
    "\n",
    "    sentence_per_line = []\n",
    "    sent = []\n",
    "    \n",
    "    for part in raw:\n",
    "        if part[-1:] != \"\\n\": # append the inner token to the sentence\n",
    "            sent.append(part)\n",
    "    \n",
    "        else:\n",
    "            sent.append(part[:-1]) # append the last word of the sentence to the sentence\n",
    "                                   # without the newline character\n",
    "            sentence_per_line.append(sent) # append the whole sentence to the list of all sentences\n",
    "            sent = [] # reset the variable for next sentence\n",
    "    \n",
    "    return sentence_per_line\n",
    "\n",
    "\n",
    "def tokenize(sentence_per_line):\n",
    "    \"\"\"Tokenize the sentences according to the rules mentioned below.\"\"\"\n",
    "\n",
    "    abbr = re.compile(r'\\b(?:[A-Za-z]\\.)+|\\b(?:[A-Z][a-z]{1,2}\\.)+|(?:[A-Z]\\.)+') # matches abbreviations such as e.g., Dr., Mrs., or U.S.A.\n",
    "    numbers = re.compile(r'(\\d+[.,]?\\d+)+') # matches decimal numbers with decimal period, or comma, or both (e.g. 2,000.3)\n",
    "    end_punct = re.compile(r'(\\.|\\?|\\!)$') # matches .?! punctuation marks (for the end of sentences)\n",
    "    end_br = re.compile(r'[\\.\\?\\!]+[\\)\\'\"]$') # matches .?! punctuation inside brackets and quotation marks\n",
    "    brq_front = re.compile(r'^[\\\"\\[\\(\\{\\']') # matches brackets and quotation marks at the beginning of the word\n",
    "    brq_back = re.compile(r'[\\\"\\]\\)\\}\\']$') # matches brackets and quotations marks at the end of the word\n",
    "\n",
    "    for lst in sentence_per_line:\n",
    "        for tok in lst:\n",
    "            if re.search(end_punct, tok) and len(tok) > 1 and not re.search(abbr, tok):\n",
    "                token1 = tok[:-1] # separate the word from the punctuation mark\n",
    "                end = tok[-1] # the punctuation mark itself\n",
    "                lst.remove(tok) # remove the last word\n",
    "                lst.append(token1) # append the separated word instead\n",
    "                lst.append(end) # append the separated punctuation instead\n",
    "\n",
    "            elif re.search(r'\\,$', tok) and len(tok) > 1:\n",
    "                where = lst.index(tok) # keep an index where to put the comma\n",
    "                token1 = tok[:-1] # the word itself\n",
    "                end = tok[-1] # the comma itself\n",
    "                lst.remove(tok) # remove the last word\n",
    "                lst.insert(where, token1) # insert the word at the index\n",
    "                lst.insert(where+1, end) # insert the comma after the word\n",
    "\n",
    "            elif re.search(end_br, tok) and len(tok) > 1: # search for punctuation inside brackets\n",
    "                token1 = tok[:-2] # word\n",
    "                end1 = tok[-2] # punctuation\n",
    "                end2 = tok[-1] # ending bracket\n",
    "                lst.remove(tok) # remove the last word\n",
    "                lst.append(token1) # append the separated word instead\n",
    "                lst.append(end1) # append the separated punctuation mark\n",
    "                lst.append(end2) # append the separated bracket\n",
    "        \n",
    "            elif re.search(brq_front, tok) and len(tok) > 1: # front brackets or quotations\n",
    "                where = lst.index(tok) # where to insert (it can be in the middle of the sentence)\n",
    "                token2 = tok[1:]\n",
    "                brq = tok[0]\n",
    "                lst.remove(tok)\n",
    "                lst.insert(where, brq) # we first want to insert the front bracket or quotation\n",
    "                lst.insert(where+1, token2) # then insert the word\n",
    "        \n",
    "            elif re.search(brq_back, tok) and len(tok) > 1: # back brackets or quotations\n",
    "                where = lst.index(tok) # where to insert\n",
    "                token1 = tok[:-1]\n",
    "                brq = tok[-1]\n",
    "                lst.remove(tok)\n",
    "                lst.insert(where, token1) # here we want the word first\n",
    "                lst.insert(where+1, brq) # and the bracket or the quote last\n",
    "\n",
    "    return sentence_per_line\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        inp = input(\"Enter a text to tokenize: \")\n",
    "        \n",
    "        if inp[-4:] != \".txt\": # the file needs to be .txt\n",
    "            print(\"NotTxtFileError: Enter a .txt file.\")\n",
    "            return 1\n",
    "        \n",
    "        raw = prepare_raw(inp)\n",
    "        newline_appended = newline_char(raw)\n",
    "        sentence_per_line = sentence_list(newline_appended)\n",
    "        tokens = tokenize(sentence_per_line)\n",
    "\n",
    "        for sentences in tokens: # print tokens divided by space\n",
    "            for toks in sentences:\n",
    "                print(toks, end=\" \")\n",
    "            print()\n",
    "\n",
    "        return 0\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"FileNotFoundError: Enter an existing file.\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
